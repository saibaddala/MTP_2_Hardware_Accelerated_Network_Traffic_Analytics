{"cells":[{"cell_type":"code","execution_count":null,"id":"60aa5ba8-ef90-454b-941e-f233251dd3ca","metadata":{"id":"60aa5ba8-ef90-454b-941e-f233251dd3ca"},"outputs":[],"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.regularizers import l1\n","from qkeras.qlayers import QDense, QActivation\n","from qkeras.quantizers import quantized_bits, quantized_relu\n","from tensorflow.keras.layers import Activation, Dropout, Dense\n","from tensorflow_model_optimization.sparsity.keras import strip_pruning\n","from tensorflow.keras.callbacks import EarlyStopping\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from sklearn.metrics import accuracy_score, classification_report\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import hls4ml\n","import time\n","\n","# Load and preprocess dataset\n","df = pd.read_csv('Dataset.csv')\n","le = LabelEncoder()\n","df['class1'] = le.fit_transform(df['class1'])\n","\n","X = df.drop(columns=['class1'])\n","y = df['class1']\n","\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",")\n","\n","print(\"✅ Preprocessing done. Shapes:\")\n","print(\"Train:\", X_train.shape, \"Test:\", X_test.shape)\n","\n","# Define QKeras model\n","model = Sequential()\n","model.add(QDense(\n","    64,\n","    input_shape=(X_train.shape[1],),\n","    name='fc1',\n","    kernel_quantizer=quantized_bits(8, 0, alpha=1),\n","    bias_quantizer=quantized_bits(8, 0, alpha=1),\n","    kernel_initializer='lecun_uniform',\n","    kernel_regularizer=l1(0.0001),\n","))\n","model.add(QActivation(activation=quantized_relu(6), name='relu1'))\n","model.add(Dropout(0.3))\n","\n","model.add(QDense(\n","    32,\n","    name='fc2',\n","    kernel_quantizer=quantized_bits(8, 0, alpha=1),\n","    bias_quantizer=quantized_bits(8, 0, alpha=1),\n","    kernel_initializer='lecun_uniform',\n","    kernel_regularizer=l1(0.0001),\n","))\n","model.add(Activation(activation='softmax', name='relu2'))\n","model.add(Dropout(0.3))\n","\n","model.add(QDense(\n","    len(set(y)),\n","    name='output',\n","    kernel_quantizer=quantized_bits(8, 0, alpha=1),\n","    bias_quantizer=quantized_bits(8, 0, alpha=1),\n","    kernel_initializer='lecun_uniform',\n","    kernel_regularizer=l1(0.0001),\n","))\n","model.add(Activation(activation='softmax', name='softmax'))\n","model.add(Dropout(0.3))\n","\n","model.summary()\n","\n","# Compile and train\n","model.compile(optimizer='nadam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","history = model.fit(\n","    X_train,\n","    y_train,\n","    epochs=50,\n","    batch_size=128,\n","    validation_split=0.2,\n","    callbacks=[early_stop],\n","    verbose=2\n",")\n","\n","# Evaluate original model\n","y_keras = model.predict(X_test)\n","y_pred_classes = np.argmax(y_keras, axis=1)\n","accuracy = accuracy_score(y_test, y_pred_classes)\n","print(f\"✅ Test Accuracy: {accuracy:.4f}\")\n","\n","# hls4ml configuration\n","config = hls4ml.utils.config_from_keras_model(model, granularity='name')\n","config['LayerName']['softmax']['exp_table_t'] = 'ap_fixed<4,1>'\n","config['LayerName']['softmax']['inv_table_t'] = 'ap_fixed<4,1>'\n","config['Model']['Strategy'] = 'Resource'\n","config['LayerName']['fc1']['ReuseFactor'] = 52\n","config['LayerName']['fc2']['ReuseFactor'] = 64\n","config['LayerName']['output']['ReuseFactor'] = 32\n","\n","for layer in ['fc1', 'fc2', 'output']:\n","    config['LayerName'][layer]['Precision']['weight'] = 'ap_fixed<4,1>'\n","    config['LayerName'][layer]['Precision']['bias'] = 'ap_fixed<4,1>'\n","\n","hls_model = hls4ml.converters.convert_from_keras_model(\n","    model,\n","    hls_config=config,\n","    io_type='io_stream',\n","    output_dir='1/hls4ml_prj_pynq',\n","    backend='VivadoAccelerator',\n","    board='pynq-z2',\n","    part='xc7z020clg400-1'\n",")\n","hls_model.compile()\n","\n","# Predict with hls model and hardware results\n","y_hls = hls_model.predict(X_test)\n","y_hw = np.load('y_hw.npy')\n","\n","y_pred_hls_classes = np.argmax(y_hls, axis=1)\n","y_pred_hls4ml_classes = np.argmax(y_hw, axis=1)\n","\n","accuracy_hls = accuracy_score(y_test, y_pred_hls_classes)\n","accuracy_hls4ml = accuracy_score(y_test, y_pred_hls4ml_classes)\n","\n","print(f\"✅ Test Accuracy of hls : {accuracy_hls:.4f}\")\n","print(f\"✅ Test Accuracy of hls : {accuracy_hls4ml:.4f}\")\n","\n","np.save('x_test.npy', X_train)\n","np.save('y_test.npy', y_test)\n","\n","print(X_train.shape)\n","print(y_train.shape)\n","\n","# HLS Build\n","start = time.time()\n","hls_model.build(\n","    csim=False,\n","    export=True,\n","    bitfile=True\n",")\n","end = time.time()\n","print(end - start)\n","\n","hls4ml.report.read_vivado_report('hls_project/hls4ml_prj_pynq')\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}